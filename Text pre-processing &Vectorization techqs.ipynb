{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f36cad",
   "metadata": {},
   "source": [
    "# Working with Text Data - Bag of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6cd04",
   "metadata": {},
   "source": [
    "## Pre-Processing steps & Convert Text to Numerical Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35538d",
   "metadata": {},
   "source": [
    "### Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153d09e",
   "metadata": {},
   "source": [
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. Some of the major application areas of NLP are:\n",
    "\n",
    "    1. Spell Checker, Keyword Search etc,\n",
    "    2. Sentiment Analysis, Spam Classification\n",
    "    3. Machine Translation\n",
    "    4. Chatbots/Dialog Systems\n",
    "    5. Question Answering Systems etc..,\n",
    "    \n",
    "However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86819c7",
   "metadata": {},
   "source": [
    "### Why NLP is hard?\n",
    "\n",
    "  1. Complexity of representation\n",
    "  2. Ambiguity in Natural Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31944b2f",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "  1. Tokenisation\n",
    "  2. Removing special characters\n",
    "  3. Convert sentence into lower case\n",
    "  4. Removing stop words\n",
    "  5. Stemming or Lemmatization  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714cd02",
   "metadata": {},
   "source": [
    "### Techniques to convert Text to Numerical Vectors\n",
    "  1. Bag of Words\n",
    "  2. TF IDF (Term Frequency - Inverse Document Frequency)\n",
    "  3. Word2Vec (by Google)\n",
    "  4. GloVe (Global Vectors by Stanford) \n",
    "  5. Pretrained GloVe Embeddings\n",
    "  6. FastText (by Facebook) \n",
    "  7. ELMo (Embeddings from Language Models) \n",
    "  8. BERT (Bidirectional Encoder Representations from Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47956b",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f733eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92297a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of 23times.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE %age of foolishness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text\n",
       "0      it Was the best oF Times $\n",
       "1    It was The worst of 23times.\n",
       "2      IT 9 was tHe age Of wisdom\n",
       "3  it was thE %age of foolishness"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_text = ['it Was the best oF Times $', \n",
    "            'It was The worst of 23times.',\n",
    "            'IT 9 was tHe age Of wisdom', \n",
    "            'it was thE %age of foolishness']\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7c0007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jessy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jessy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jessy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Downloading wordnet before applying Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e442fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05d3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb35ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use Lemmatizer instead of Stemmer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb336c69",
   "metadata": {},
   "source": [
    "### Text Preprocessing Steps\n",
    "\n",
    "Text Preprocessing steps include some essential tasks to clean and remove the noise from the available data.\n",
    "\n",
    "  1. ***Removing Special Characters and Punctuation***\n",
    "  \n",
    "\n",
    "  2. ***Converting to Lower Case -*** We convert the whole text corpus to lower case to reduce the size of the vocabulary of our text data.\n",
    "  \n",
    "\n",
    "  3. ***Removing Stop Words -*** Stopwords don't contribute to the meaning of a sentence. So, we can safely remove them without changing the meaning of the sentence. For eg: it, was, any, then, a, is, by, etc are the stopwords.\n",
    "  \n",
    "\n",
    "  4. ***Stemming or Lemmatization -*** Stemming is the process of getting the root form of a word. For eg: warm, warmer, warming can be converted to warm.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb7f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 1is Natural-LAnguage-Processing.& This is @Text pre-Processing4\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"This 1is Natural-LAnguage-Processing.& This is @Text pre-Processing4\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428f7d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  is Natural LAnguage Processing   This is  Text pre Processing \n"
     ]
    }
   ],
   "source": [
    "# Removing special characters and digits\n",
    "sentence = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "073cd48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this  is natural language processing   this is  text pre processing \n"
     ]
    }
   ],
   "source": [
    "# change sentence to lower case\n",
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abfdbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'natural', 'language', 'processing', 'this', 'is', 'text', 'pre', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c8ec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'text', 'pre', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "clean_tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d558cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'text', 'pre', 'process']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in clean_tokens]\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462769ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'text', 'pre', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334952e7",
   "metadata": {},
   "source": [
    "## Lets put it all together in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55707ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text, flag):\n",
    "    # Removing special characters and digits\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    \n",
    "    # change sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # remove stop words                \n",
    "    clean_tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Stemming/Lemmatization\n",
    "    if(flag == 'stem'):\n",
    "        clean_tokens = [stemmer.stem(word) for word in clean_tokens]\n",
    "    else:\n",
    "        clean_tokens = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "    \n",
    "    return pd.Series([\" \".join(clean_tokens), len(clean_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f2a771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1\n",
       "0    best time  2\n",
       "1   worst time  2\n",
       "2   age wisdom  2\n",
       "3  age foolish  2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x : preprocess(x, 'stem'))\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fb7ee3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_stem  text_length_stem\n",
       "0       best time                 2\n",
       "1      worst time                 2\n",
       "2      age wisdom                 2\n",
       "3     age foolish                 2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_stem', 'text_length_stem']\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed4bad59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of 23times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE %age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text clean_text_stem  text_length_stem\n",
       "0      it Was the best oF Times $       best time                 2\n",
       "1    It was The worst of 23times.      worst time                 2\n",
       "2      IT 9 was tHe age Of wisdom      age wisdom                 2\n",
       "3  it was thE %age of foolishness     age foolish                 2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3eeb5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1\n",
       "0        best time  2\n",
       "1       worst time  2\n",
       "2       age wisdom  2\n",
       "3  age foolishness  2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x: preprocess(x, 'lemma'))\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e75941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_lemma  text_length_lemma\n",
       "0        best time                  2\n",
       "1       worst time                  2\n",
       "2       age wisdom                  2\n",
       "3  age foolishness                  2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_lemma', 'text_length_lemma']\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c91a927e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of 23times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE %age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text clean_text_stem  text_length_stem  \\\n",
       "0      it Was the best oF Times $       best time                 2   \n",
       "1    It was The worst of 23times.      worst time                 2   \n",
       "2      IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE %age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3  age foolishness                  2  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f6265",
   "metadata": {},
   "source": [
    "# Bag of Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29676904",
   "metadata": {},
   "source": [
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This\n",
    "\n",
    "specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation.\n",
    "\n",
    "Documents are described by word occurrences while completely ignoring the relative position information of the words in the \n",
    "\n",
    "document.\n",
    "\n",
    "\n",
    "We will use `CountVectorizer` to convert ***text into a matrix of token count.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ffa34",
   "metadata": {},
   "source": [
    "`Bag of Words`: https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "`Code Example`: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c641286",
   "metadata": {},
   "source": [
    "***We are going to perform below mentioned steps to understand the entire process:***\n",
    "\n",
    " 1. Converting text to numerical vectors with the help of `CountVectorizer`\n",
    " \n",
    " 2. Understand `fit` and `transform`\n",
    " \n",
    " 3. Looking at `vocabulary_`\n",
    " \n",
    " 4. Converting `sparse matrix` to dense matrix using `toarray()`\n",
    " \n",
    " 5. Understanding `n_gram`\n",
    " \n",
    " \n",
    " \n",
    "### Advantages\n",
    "\n",
    " 1. It is simple to understand and implement like OneHotEncoding.\n",
    " \n",
    " \n",
    " 2. We have a fixed length encoding for any sequence of arbitrary length.\n",
    " \n",
    " \n",
    " 3. Documents with same words/vocabulary will have similar representation. So if two documents have a similar vocabulary,           they’ll be closer to each other in the vector space and vice versa.\n",
    " \n",
    " \n",
    "### Disadvantages\n",
    "\n",
    "1. The size of vector increases with the size of the vocabulary. Thus, sparsity continues to be a problem. One way to control it    is by limiting the vocabulary to n number of the most frequent words.\n",
    "\n",
    "\n",
    "2. It does not capture the similarity between different words that mean the same thing.i.e.`Semantic Meaning` is not                  captured.\n",
    "\n",
    "   > a. \"walk\", \"walked\", and \"walking\". BoW vectors of all three tokens will be equally apart.\n",
    "    \n",
    "   > b. \"search\" and \"explore\" are synonyms. BoW won't capture the semantic similarity of these words.\n",
    "\n",
    "\n",
    "3. This representation does not have any way to handle `out of vocabulary (OOV)`words (i.e., new words that were not seen in the     corpus that was used to build the vectorizer).\n",
    "\n",
    "\n",
    "4. As the name indicates, it is a “bag” of words. Word order information is lost in this representation. One way to control it \n",
    "   is by using `n-grams`.\n",
    "   \n",
    "   \n",
    "5. It suffers from ***curse of high dimensionality.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b54a7f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of 23times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE %age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text clean_text_stem  text_length_stem  \\\n",
       "0      it Was the best oF Times $       best time                 2   \n",
       "1    It was The worst of 23times.      worst time                 2   \n",
       "2      IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE %age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3  age foolishness                  2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4aa142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vocab = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "dtm = vocab.fit_transform(df['clean_text_lemma'])\n",
    "\n",
    "# fit_transform() could be done seperatly as mentioned below\n",
    "# vocab.fit(df.clean_text_stem)\n",
    "# dtm = vocab.transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aea31f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc66196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Observe that the type of dtm is sparse\n",
    "\n",
    "print(type(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fd4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "# Lets now print the  shape of this dtm\n",
    "\n",
    "print(dtm.shape)\n",
    "\n",
    "# o/p -> (4, 6)\n",
    "# i.e -> 4 documents and 6 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895fdb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the dtm\n",
    "\n",
    "print(dtm)\n",
    "\n",
    "# Remember that dtm is a sparse matrix. i.e. zeros wont be stored\n",
    "# Lets understand First line of output -> (0,1)    1\n",
    "# Here (0, 1) means 0th document and 1st(index starting from 0) unique word. \n",
    "# (we have total 4 documents) & (we have total 6 unique words)\n",
    "# (0, 1)    1 -> 1 here refers to the number of occurence of 1st word\n",
    "# Now lets read it all in english.\n",
    "# (0, 1)    1 -> 'times' occurs 1 time in 0th document. \n",
    "# Try to observe -> (3, 2)   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6e514ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0]\n",
      " [0 0 0 1 0 1]\n",
      " [1 0 0 0 1 0]\n",
      " [1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Since the dtm is sparse, lets convert it into numpy array.\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3abb4d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'best', 'foolishness', 'time', 'wisdom', 'worst']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39afa003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  best  foolishness  time  wisdom  worst\n",
       "0    0     1            0     1       0      0\n",
       "1    0     0            0     1       0      1\n",
       "2    1     0            0     0       1      0\n",
       "3    1     0            1     0       0      0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c25b9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "\n",
    "vocab = CountVectorizer(ngram_range=[1,2])\n",
    "\n",
    "dtm = vocab.fit_transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50ec445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 3, 'time': 6, 'best time': 4, 'worst': 8, 'worst time': 9, 'age': 0, 'wisdom': 7, 'age wisdom': 2, 'foolish': 5, 'age foolish': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9bb738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 1]\n",
      " [1 0 1 0 0 0 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21017189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age foolish</th>\n",
       "      <th>age wisdom</th>\n",
       "      <th>best</th>\n",
       "      <th>best time</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "      <th>worst time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  age foolish  age wisdom  best  best time  foolish  time  wisdom  \\\n",
       "0    0            0           0     1          1        0     1       0   \n",
       "1    0            0           0     0          0        0     1       0   \n",
       "2    1            0           1     0          0        0     0       1   \n",
       "3    1            1           0     0          0        1     0       0   \n",
       "\n",
       "   worst  worst time  \n",
       "0      0           0  \n",
       "1      1           1  \n",
       "2      0           0  \n",
       "3      0           0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5851fa",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "1. `vect.fit(lst_text)` **learns the vocabulary**\n",
    "2. `vect.transform(lst_text)` **uses the fitted vocabulary to build a document-term matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091b60e",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "attachments": {
    "gif.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b295c9c5",
   "metadata": {},
   "source": [
    "In BOW approach all the words in the text are treated as equally important i.e. there's no notion of some words in the document\n",
    "\n",
    "being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to \n",
    "\n",
    "quantify the importance of a given word relative to other words in the document and in the corpus.\n",
    "\n",
    "***\n",
    "\n",
    "Let's now try to understand:\n",
    "\n",
    "1. Term Frequency\n",
    "2. Inverse Document Frequency\n",
    "\n",
    "\n",
    "![gif.gif](attachment:gif.gif)\n"
   ]
  },
  {
   "attachments": {
    "gif-2.gif": {
     "image/gif": "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"
    },
    "gif.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "id": "1031c0eb",
   "metadata": {},
   "source": [
    "![gif.gif](attachment:gif.gif)\n",
    "\n",
    "![gif-2.gif](attachment:gif-2.gif)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f44afb",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "1. If the word is rare in the corpus, it will be given more importance. (i.e. IDF)\n",
    "2. If the word is more frequent in a document, it will be given more importance. (i.e. TF)\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "> ***Same as BOW***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "485604aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "dtm = vectorizer.fit_transform(df.clean_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "138f2c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d00717fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.78528828 0.         0.6191303  0.         0.        ]\n",
      " [0.         0.         0.         0.6191303  0.         0.78528828]\n",
      " [0.6191303  0.         0.         0.         0.78528828 0.        ]\n",
      " [0.6191303  0.         0.78528828 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to nparray\n",
    "print(dtm.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdbba97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age      best  foolishness     time    wisdom     worst\n",
       "0  0.00000  0.785288     0.000000  0.61913  0.000000  0.000000\n",
       "1  0.00000  0.000000     0.000000  0.61913  0.000000  0.785288\n",
       "2  0.61913  0.000000     0.000000  0.00000  0.785288  0.000000\n",
       "3  0.61913  0.000000     0.785288  0.00000  0.000000  0.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabdda9",
   "metadata": {},
   "source": [
    "## Latent Space\n",
    "\n",
    "\n",
    "A latent space, also known as a latent feature space or embedding space, is an embedding of a set of items within a manifold in \n",
    "which items which resemble each other more closely are positioned closer to one another in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea15ba",
   "metadata": {},
   "source": [
    "## Word Embeddings (Word Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab32ddb",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), [word embedding](https://en.wikipedia.org/wiki/Word_embedding) is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of [language modeling](https://en.wikipedia.org/wiki/Language_model) and [feature learning](https://en.wikipedia.org/wiki/Feature_learning) techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "\n",
    "Methods to generate this mapping include **neural networks**, **dimensionality reduction on the word co-occurrence matrix**, **probabilistic models**, **explainable knowledge base method**, and **explicit representation in terms of the context in which words appear**.\n",
    "\n",
    "\n",
    "Traditionally, one of the main **limitations of word embeddings** (word vector space models in general) is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, [polysemy](https://en.wikipedia.org/wiki/Polysemy) and [homonymy](https://en.wikipedia.org/wiki/Homonym) are not handled properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3c3fe",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"You shall know the word by the company it keeps.\" by JR Firth\n",
    "\n",
    "***Distributional Semantics (i.e. a word is characterized by the company it keeps)***\n",
    "\n",
    "W2v works well because there is an idea of meaning distribution in the context.\n",
    "\n",
    "\n",
    "***Algorithms to generate Word2Vec Embeddings***\n",
    "\n",
    " 1. SkipGram\n",
    " 2. Continuous Bag of Words\n",
    " \n",
    "***Issue***\n",
    "\n",
    "Even if the word is having three different meaning, W2v will return the weighted average of all three as the output. Now the question is,\n",
    "\n",
    " - Is it possible to segregate the three vectors to represent the words based in the context?\n",
    " - Is it possible to disambiguate the word vectors based on the context?\n",
    "\n",
    "Word2Vec is not capturing the contextual information. This is where BERT comes handy.\n",
    "\n",
    "**`! pip install gensim`**\n",
    "\n",
    "**`! pip install --upgrade gensim`**\n",
    "\n",
    "Run this in command promp (admin mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56e47518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\jessy\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\jessy\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jessy\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jessy\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\jessy\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "Successfully installed gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a216eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ce2d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3942e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of 23times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE %age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text clean_text_stem  text_length_stem  \\\n",
       "0      it Was the best oF Times $       best time                 2   \n",
       "1    It was The worst of 23times.      worst time                 2   \n",
       "2      IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE %age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3  age foolishness                  2      [age, foolish]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenised_sentences'] = df.clean_text_stem.apply(lambda sent : sent.split())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "706f8346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['best', 'time'], ['worst', 'time'], ['age', 'wisdom'], ['age', 'foolish']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.tokenised_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5618a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "model = Word2Vec(list(df.tokenised_sentences), vector_size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84c5ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94dbb3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Documents\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46284616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0, 'time': 1, 'foolish': 2, 'wisdom': 3, 'worst': 4, 'best': 5}\n",
      "['age', 'time', 'foolish', 'wisdom', 'worst', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Looking at the vocabulary\n",
    "\n",
    "print(model.wv.key_to_index)\n",
    "\n",
    "print(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6213584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.4563962e-05  3.0773187e-03 -6.8126465e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464084e-03 -3.6732983e-03  2.6427007e-03\n",
      " -8.3171297e-03  6.2054847e-03 -4.6373224e-03 -3.1641079e-03\n",
      "  9.3113566e-03  8.7338447e-04  7.4907015e-03 -6.0740639e-03\n",
      "  5.1605059e-03  9.9228211e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648384e-03 -4.8626517e-03 -3.7785650e-03 -8.5362010e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236125e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578700e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257271e-03 -1.5815735e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418793e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936899e-03\n",
      "  3.9735888e-03  4.5294715e-03  1.4343048e-03 -2.6998566e-03\n",
      " -4.3668128e-03 -1.0320758e-03  1.4370275e-03 -2.6460099e-03\n",
      " -7.0737838e-03 -7.8053069e-03 -9.1217877e-03 -5.9351707e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606713e-03 -3.7173224e-03\n",
      "  4.2891572e-03 -3.7390448e-03  8.3781742e-03  1.5339922e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312111e-03  5.4932809e-03\n",
      " -6.8488456e-03  5.8226776e-03  4.0090918e-03  5.1853680e-03\n",
      "  4.2559002e-03  1.9397545e-03 -3.1701636e-03  8.3538434e-03\n",
      "  9.6121784e-03  3.7926030e-03 -2.8369951e-03  7.1263312e-06\n",
      "  1.2188172e-03 -8.4583256e-03 -8.2239462e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433820e-03 -4.7252751e-03 -7.3460746e-03\n",
      "  8.3286138e-03  1.2129784e-04 -4.5093987e-03  5.7017040e-03\n",
      "  9.1800140e-03 -4.0998720e-03  7.9646800e-03  5.3754328e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# access the 100 dimensional vector for one of the words\n",
    "\n",
    "print(model.wv.__getitem__('foolish'))\n",
    "\n",
    "print(model.wv.__getitem__('foolish').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dd08155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.3622725e-04  2.3643016e-04  5.1033497e-03  9.0092728e-03\n",
      "  -9.3029495e-03 -7.1168090e-03  6.4588715e-03  8.9729885e-03\n",
      "  -5.0154282e-03 -3.7633730e-03  7.3805046e-03 -1.5334726e-03\n",
      "  -4.5366143e-03  6.5540504e-03 -4.8601604e-03 -1.8160177e-03\n",
      "   2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488189e-03\n",
      "   7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "   6.3508893e-03 -3.4053659e-03 -9.4640255e-04  5.7685734e-03\n",
      "  -7.5216386e-03 -3.9361049e-03 -7.5115822e-03 -9.3004224e-04\n",
      "   9.5381187e-03 -7.3191668e-03 -2.3337698e-03 -1.9377422e-03\n",
      "   8.0774352e-03 -5.9308959e-03  4.5161247e-05 -4.7537349e-03\n",
      "  -9.6035507e-03  5.0072931e-03 -8.7595871e-03 -4.3918253e-03\n",
      "  -3.5099984e-05 -2.9618264e-04 -7.6612402e-03  9.6147414e-03\n",
      "   4.9820566e-03  9.2331432e-03 -8.1579182e-03  4.4957972e-03\n",
      "  -4.1370774e-03  8.2453492e-04  8.4986184e-03 -4.4621779e-03\n",
      "   4.5175003e-03 -6.7869616e-03 -3.5484887e-03  9.3985079e-03\n",
      "  -1.5776539e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      "  -1.5080094e-03  2.4697948e-03 -8.8802812e-04  5.5336617e-03\n",
      "  -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459523e-03\n",
      "  -1.4537406e-03 -9.2081428e-03  4.3705511e-03  5.7178497e-04\n",
      "   7.4419067e-03 -8.1328390e-04 -2.6384138e-03 -8.7530091e-03\n",
      "  -8.5655687e-04  2.8265619e-03  5.4014279e-03  7.0526553e-03\n",
      "  -5.7031228e-03  1.8588186e-03  6.0888622e-03 -4.7980524e-03\n",
      "  -3.1072616e-03  6.7976285e-03  1.6314745e-03  1.8991709e-04\n",
      "   3.4736372e-03  2.1777629e-04  9.6188262e-03  5.0606038e-03\n",
      "  -8.9173913e-03 -7.0415614e-03  9.0145587e-04  6.3925339e-03]\n",
      " [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "   7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      "  -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      "  -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "   6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "   2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "   6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      "  -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      "  -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "   6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "   7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      "  -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "   1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      "  -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "   9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      "  -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "   3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "   7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "   5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      "  -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      "  -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "   1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "   2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "   2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "   5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n",
      " [ 9.4563962e-05  3.0773187e-03 -6.8126465e-03 -1.3754654e-03\n",
      "   7.6685809e-03  7.3464084e-03 -3.6732983e-03  2.6427007e-03\n",
      "  -8.3171297e-03  6.2054847e-03 -4.6373224e-03 -3.1641079e-03\n",
      "   9.3113566e-03  8.7338447e-04  7.4907015e-03 -6.0740639e-03\n",
      "   5.1605059e-03  9.9228211e-03 -8.4573915e-03 -5.1356913e-03\n",
      "  -7.0648384e-03 -4.8626517e-03 -3.7785650e-03 -8.5362010e-03\n",
      "   7.9556061e-03 -4.8439382e-03  8.4236125e-03  5.2625705e-03\n",
      "  -6.5500261e-03  3.9578700e-03  5.4701497e-03 -7.4265362e-03\n",
      "  -7.4057197e-03 -2.4752307e-03 -8.6257271e-03 -1.5815735e-03\n",
      "  -4.0343284e-04  3.2996845e-03  1.4418793e-03 -8.8142155e-04\n",
      "  -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936899e-03\n",
      "   3.9735888e-03  4.5294715e-03  1.4343048e-03 -2.6998566e-03\n",
      "  -4.3668128e-03 -1.0320758e-03  1.4370275e-03 -2.6460099e-03\n",
      "  -7.0737838e-03 -7.8053069e-03 -9.1217877e-03 -5.9351707e-03\n",
      "  -1.8474245e-03 -4.3238713e-03 -6.4606713e-03 -3.7173224e-03\n",
      "   4.2891572e-03 -3.7390448e-03  8.3781742e-03  1.5339922e-03\n",
      "  -7.2423196e-03  9.4337985e-03  7.6312111e-03  5.4932809e-03\n",
      "  -6.8488456e-03  5.8226776e-03  4.0090918e-03  5.1853680e-03\n",
      "   4.2559002e-03  1.9397545e-03 -3.1701636e-03  8.3538434e-03\n",
      "   9.6121784e-03  3.7926030e-03 -2.8369951e-03  7.1263312e-06\n",
      "   1.2188172e-03 -8.4583256e-03 -8.2239462e-03 -2.3101569e-04\n",
      "   1.2372875e-03 -5.7433820e-03 -4.7252751e-03 -7.3460746e-03\n",
      "   8.3286138e-03  1.2129784e-04 -4.5093987e-03  5.7017040e-03\n",
      "   9.1800140e-03 -4.0998720e-03  7.9646800e-03  5.3754328e-03\n",
      "   5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      " [-8.2426788e-03  9.2993546e-03 -1.9766092e-04 -1.9672776e-03\n",
      "   4.6036290e-03 -4.0953159e-03  2.7431131e-03  6.9399667e-03\n",
      "   6.0654259e-03 -7.5107957e-03  9.3823504e-03  4.6718074e-03\n",
      "   3.9661191e-03 -6.2435055e-03  8.4599778e-03 -2.1501661e-03\n",
      "   8.8251876e-03 -5.3620026e-03 -8.1294207e-03  6.8245577e-03\n",
      "   1.6711927e-03 -2.1985101e-03  9.5135998e-03  9.4938539e-03\n",
      "  -9.7740479e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "   2.0227861e-03  4.3050051e-04  6.7363022e-04 -3.8206363e-03\n",
      "  -7.1402504e-03 -2.0888734e-03  3.9238976e-03  8.8186832e-03\n",
      "   9.2591504e-03 -5.9759379e-03 -9.4026709e-03  9.7643761e-03\n",
      "   3.4297847e-03  5.1661157e-03  6.2823440e-03 -2.8042626e-03\n",
      "   7.3227026e-03  2.8302716e-03  2.8710032e-03 -2.3803711e-03\n",
      "  -3.1282497e-03 -2.3701428e-03  4.2764354e-03  7.6057913e-05\n",
      "  -9.5842788e-03 -9.6655441e-03 -6.1481954e-03 -1.2856961e-04\n",
      "   1.9974159e-03  9.4319675e-03  5.5843499e-03 -4.2906976e-03\n",
      "   2.7831554e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "   4.3234206e-03 -5.8143805e-03 -8.0419064e-04  8.1000496e-03\n",
      "  -2.3600650e-03 -9.6634552e-03  5.7792594e-03 -3.9298222e-03\n",
      "  -1.2228728e-03  9.9805165e-03 -2.2563506e-03 -4.7570658e-03\n",
      "  -5.3293873e-03  6.9808890e-03 -5.7088733e-03  2.1136617e-03\n",
      "  -5.2556610e-03  6.1207130e-03  4.3573068e-03  2.6063537e-03\n",
      "  -1.4910841e-03 -2.7460647e-03  8.9929365e-03  5.2157734e-03\n",
      "  -2.1625208e-03 -9.4703101e-03 -7.4260519e-03 -1.0637427e-03\n",
      "  -7.9494715e-04 -2.5629092e-03  9.6827196e-03 -4.5852186e-04\n",
      "   5.8737611e-03 -7.4475883e-03 -2.5060750e-03 -5.5498648e-03]\n",
      " [-7.1390150e-03  1.2410306e-03 -7.1767163e-03 -2.2446180e-03\n",
      "   3.7193035e-03  5.8331229e-03  1.1981821e-03  2.1027303e-03\n",
      "  -4.1103913e-03  7.2253323e-03 -6.3070417e-03  4.6472144e-03\n",
      "  -8.2199741e-03  2.0364665e-03 -4.9770521e-03 -4.2476892e-03\n",
      "  -3.1089855e-03  5.6552077e-03  5.7983994e-03 -4.9746488e-03\n",
      "   7.7332975e-04 -8.4957788e-03  7.8098057e-03  9.2572905e-03\n",
      "  -2.7423287e-03  8.0022332e-04  7.4665068e-04  5.4778839e-03\n",
      "  -8.6060790e-03  5.8445451e-04  6.8694209e-03  2.2315932e-03\n",
      "   1.1246753e-03 -9.3221571e-03  8.4823659e-03 -6.2641287e-03\n",
      "  -2.9923748e-03  3.4937859e-03 -7.7262876e-04  1.4112901e-03\n",
      "   1.7819905e-03 -6.8288995e-03 -9.7248126e-03  9.0405848e-03\n",
      "   6.1980532e-03 -6.9129276e-03  3.4034825e-03  2.0606279e-04\n",
      "   4.7537447e-03 -7.1199443e-03  4.0269541e-03  4.3474343e-03\n",
      "   9.9573685e-03 -4.4737412e-03 -1.3892651e-03 -7.3173214e-03\n",
      "  -9.6978303e-03 -9.0802573e-03 -1.0227561e-03 -6.5032910e-03\n",
      "   4.8497273e-03 -6.1640264e-03  2.5191857e-03  7.3944090e-04\n",
      "  -3.3921553e-03 -9.7922329e-04  9.9791242e-03  9.1458866e-03\n",
      "  -4.4618296e-03  9.0830252e-03 -5.6417631e-03  5.9309220e-03\n",
      "  -3.0972194e-03  3.4317516e-03  3.0172253e-03  6.9004609e-03\n",
      "  -2.3738837e-03  8.7750368e-03  7.5894282e-03 -9.5476462e-03\n",
      "  -8.0082109e-03 -7.6378966e-03  2.9232574e-03 -2.7947235e-03\n",
      "  -6.9295214e-03 -8.1282640e-03  8.3091781e-03  1.9904876e-03\n",
      "  -9.3280170e-03 -4.7927164e-03  3.1367373e-03 -4.7132061e-03\n",
      "   5.2808430e-03 -4.2334413e-03  2.6417947e-03 -8.0456873e-03\n",
      "   6.2098862e-03  4.8188879e-03  7.8719138e-04  3.0134462e-03]\n",
      " [-8.7274835e-03  2.1301603e-03 -8.7354420e-04 -9.3190884e-03\n",
      "  -9.4281435e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      "  -6.4986944e-03 -6.8730689e-03 -4.9994136e-03 -2.2868442e-03\n",
      "  -7.2502876e-03 -9.6033188e-03 -2.7436304e-03 -8.3628418e-03\n",
      "  -6.0388758e-03 -5.6709289e-03 -2.3441387e-03 -1.7069983e-03\n",
      "  -8.9569995e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      "  -7.2061159e-03 -3.6668323e-03  3.1185509e-03 -9.5707225e-03\n",
      "   1.4764380e-03  6.5244650e-03  5.7464195e-03 -8.7630628e-03\n",
      "  -4.5171450e-03 -8.1401607e-03  4.5955181e-05  9.2636319e-03\n",
      "   5.9733056e-03  5.0673080e-03  5.0610616e-03 -3.2429171e-03\n",
      "   9.5521836e-03 -7.3564244e-03 -7.2703888e-03 -2.2653891e-03\n",
      "  -7.7856064e-04 -3.2161046e-03 -5.9258699e-04  7.4888230e-03\n",
      "  -6.9751980e-04 -1.6249418e-03  2.7443981e-03 -8.3591007e-03\n",
      "   7.8558037e-03  8.5361032e-03 -9.5840879e-03  2.4462652e-03\n",
      "   9.9049713e-03 -7.6658037e-03 -6.9669201e-03 -7.7365185e-03\n",
      "   8.3959224e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "   3.7430834e-03  2.6350426e-03  7.4271200e-04  2.3276759e-03\n",
      "  -7.4690939e-03 -9.3583753e-03  2.3545765e-03  6.1484552e-03\n",
      "   7.9856869e-03  5.7358933e-03 -7.7733753e-04  8.3061643e-03\n",
      "  -9.3363142e-03  3.4061312e-03  2.6675223e-04  3.8572431e-03\n",
      "   7.3857834e-03 -6.7251683e-03  5.5844807e-03 -9.5222257e-03\n",
      "  -8.0446003e-04 -8.6887386e-03 -5.0986744e-03  9.2892265e-03\n",
      "  -1.8582630e-03  2.9144264e-03  9.0712784e-03  8.9381309e-03\n",
      "  -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044296e-03\n",
      "  -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758990e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Access the 100D vectors for all 6 words\n",
    "\n",
    "print(model.wv.__getitem__(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a9378c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100)\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.__getitem__(model.wv.index_to_key).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ce4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# model.save('model/first_word_vectors.bin')\n",
    "\n",
    "# # load model\n",
    "# new_model = Word2Vec.load('model/first_word_vectors.bin')\n",
    "# print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3cad45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model.wv.__getitem__(model.wv.index_to_key)\n",
    "pca = PCA(n_components = 2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.index_to_key)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cae46dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13887985"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('best', 'worst')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9ea7d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.17018882930278778),\n",
       " ('best', 0.06408979743719101),\n",
       " ('wisdom', -0.013514960184693336),\n",
       " ('time', -0.02367166243493557),\n",
       " ('age', -0.052346743643283844)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('foolish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e50ecc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of 23times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE %age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text clean_text_stem  text_length_stem  \\\n",
       "0      it Was the best oF Times $       best time                 2   \n",
       "1    It was The worst of 23times.      worst time                 2   \n",
       "2      IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE %age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3  age foolishness                  2      [age, foolish]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f6820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
